

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>3.9.2.10. pyspark.ml.tuning.SparkContext &mdash; PySpark API</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/favicon-umich.ico"/>
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="PySpark API" href="../../index.html"/>
        <link rel="up" title="3.9. pyspark.ml.tuning" href="../pyspark.ml.tuning.html"/>
        <link rel="next" title="3.9.2.10.1.1. pyspark.ml.tuning.SparkContext.__init__" href="pyspark.ml.tuning.SparkContext.__init__.html"/>
        <link rel="prev" title="3.9.2.9.2.1. pyspark.ml.tuning.Params.params" href="pyspark.ml.tuning.Params.params.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> PySpark API
          

          
          </a>

          
            
            
              <div class="version">
                1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <p class="caption"><span class="caption-text">Table of Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../pyspark.html">1. pyspark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pyspark.sql.html">2. pyspark.sql</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../pyspark.ml.html">3. pyspark.ml</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../pyspark.ml.html">3.1. pyspark.ml</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pyspark.ml.param.html">3.2. pyspark.ml.param</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pyspark.ml.feature.html">3.3. pyspark.ml.feature</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pyspark.ml.classification.html">3.4. pyspark.ml.classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pyspark.ml.clustering.html">3.5. pyspark.ml.clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pyspark.ml.linalg.html">3.6. pyspark.ml.linalg</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pyspark.ml.recommendation.html">3.7. pyspark.ml.recommendation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pyspark.ml.regression.html">3.8. pyspark.ml.regression</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../pyspark.ml.tuning.html">3.9. pyspark.ml.tuning</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../pyspark.ml.tuning.html#functions">3.9.1. Functions</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../pyspark.ml.tuning.html#classes">3.9.2. Classes</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="pyspark.ml.tuning.CrossValidator.html">3.9.2.1. pyspark.ml.tuning.CrossValidator</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.ml.tuning.CrossValidatorModel.html">3.9.2.2. pyspark.ml.tuning.CrossValidatorModel</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.ml.tuning.Estimator.html">3.9.2.3. pyspark.ml.tuning.Estimator</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.ml.tuning.HasSeed.html">3.9.2.4. pyspark.ml.tuning.HasSeed</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.ml.tuning.JavaParams.html">3.9.2.5. pyspark.ml.tuning.JavaParams</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.ml.tuning.Model.html">3.9.2.6. pyspark.ml.tuning.Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.ml.tuning.Param.html">3.9.2.7. pyspark.ml.tuning.Param</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.ml.tuning.ParamGridBuilder.html">3.9.2.8. pyspark.ml.tuning.ParamGridBuilder</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.ml.tuning.Params.html">3.9.2.9. pyspark.ml.tuning.Params</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="">3.9.2.10. pyspark.ml.tuning.SparkContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.ml.tuning.TrainValidationSplit.html">3.9.2.11. pyspark.ml.tuning.TrainValidationSplit</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.ml.tuning.TrainValidationSplitModel.html">3.9.2.12. pyspark.ml.tuning.TrainValidationSplitModel</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.ml.tuning.TypeConverters.html">3.9.2.13. pyspark.ml.tuning.TypeConverters</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.ml.tuning.ValidatorParams.html">3.9.2.14. pyspark.ml.tuning.ValidatorParams</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../pyspark.ml.evaluation.html">3.10. pyspark.ml.evaluation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../pyspark.mllib.html">4. pyspark.mllib</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pyspark.streaming.html">5. pyspark.streaming</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">PySpark API</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          













<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../pyspark.ml.html">3. pyspark.ml</a> &raquo;</li>
        
          <li><a href="../pyspark.ml.tuning.html">3.9. pyspark.ml.tuning</a> &raquo;</li>
        
      <li>3.9.2.10. pyspark.ml.tuning.SparkContext</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/generated/generated/pyspark.ml.tuning.SparkContext.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="pyspark-ml-tuning-sparkcontext">
<h1>3.9.2.10. pyspark.ml.tuning.SparkContext<a class="headerlink" href="#pyspark-ml-tuning-sparkcontext" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="pyspark.ml.tuning.SparkContext">
<em class="property">class </em><code class="descclassname">pyspark.ml.tuning.</code><code class="descname">SparkContext</code><span class="sig-paren">(</span><em>master=None</em>, <em>appName=None</em>, <em>sparkHome=None</em>, <em>pyFiles=None</em>, <em>environment=None</em>, <em>batchSize=0</em>, <em>serializer=PickleSerializer()</em>, <em>conf=None</em>, <em>gateway=None</em>, <em>jsc=None</em>, <em>profiler_cls=&lt;class 'pyspark.profiler.BasicProfiler'&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/pyspark/context.html#SparkContext"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.ml.tuning.SparkContext" title="Permalink to this definition">¶</a></dt>
<dd><p>Main entry point for Spark functionality. A SparkContext represents the
connection to a Spark cluster, and can be used to create L{RDD} and
broadcast variables on that cluster.</p>
<dl class="method">
<dt id="pyspark.ml.tuning.SparkContext.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>master=None</em>, <em>appName=None</em>, <em>sparkHome=None</em>, <em>pyFiles=None</em>, <em>environment=None</em>, <em>batchSize=0</em>, <em>serializer=PickleSerializer()</em>, <em>conf=None</em>, <em>gateway=None</em>, <em>jsc=None</em>, <em>profiler_cls=&lt;class 'pyspark.profiler.BasicProfiler'&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/pyspark/context.html#SparkContext.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.ml.tuning.SparkContext.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a new SparkContext. At least the master and app name should be set,
either through the named parameters here or through C{conf}.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>master</strong> &#8211; Cluster URL to connect to
(e.g. mesos://host:port, spark://host:port, local[4]).</li>
<li><strong>appName</strong> &#8211; A name for your job, to display on the cluster web UI.</li>
<li><strong>sparkHome</strong> &#8211; Location where Spark is installed on cluster nodes.</li>
<li><strong>pyFiles</strong> &#8211; Collection of .zip or .py files to send to the cluster
and add to PYTHONPATH.  These can be paths on the local file
system or HDFS, HTTP, HTTPS, or FTP URLs.</li>
<li><strong>environment</strong> &#8211; A dictionary of environment variables to set on
worker nodes.</li>
<li><strong>batchSize</strong> &#8211; The number of Python objects represented as a single
Java object. Set 1 to disable batching, 0 to automatically choose
the batch size based on object sizes, or -1 to use an unlimited
batch size</li>
<li><strong>serializer</strong> &#8211; The serializer for RDDs.</li>
<li><strong>conf</strong> &#8211; A L{SparkConf} object setting Spark properties.</li>
<li><strong>gateway</strong> &#8211; Use an existing gateway and JVM, otherwise a new JVM
will be instantiated.</li>
<li><strong>jsc</strong> &#8211; The JavaSparkContext instance (optional).</li>
<li><strong>profiler_cls</strong> &#8211; A class of custom Profiler used to do profiling
(default is pyspark.profiler.BasicProfiler).</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.context</span> <span class="kn">import</span> <span class="n">SparkContext</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="s1">&#39;local&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span>&gt;&gt;&gt; sc2 = SparkContext(&#39;local&#39;, &#39;test2&#39;) 
Traceback (most recent call last):
    ...
ValueError:...
</pre></div>
</div>
</dd></dl>

</dd></dl>

<div class="section" id="methods">
<h2>3.9.2.10.1. Methods<a class="headerlink" href="#methods" title="Permalink to this headline">¶</a></h2>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.__init__.html#pyspark.ml.tuning.SparkContext.__init__" title="pyspark.ml.tuning.SparkContext.__init__"><code class="xref py py-obj docutils literal"><span class="pre">__init__</span></code></a>([master,&nbsp;appName,&nbsp;sparkHome,&nbsp;...])</td>
<td>Create a new SparkContext.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.accumulator.html#pyspark.ml.tuning.SparkContext.accumulator" title="pyspark.ml.tuning.SparkContext.accumulator"><code class="xref py py-obj docutils literal"><span class="pre">accumulator</span></code></a>(value[,&nbsp;accum_param])</td>
<td>Create an L{Accumulator} with the given initial value, using a given L{AccumulatorParam} helper object to define how to add values of the data type if provided.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.addFile.html#pyspark.ml.tuning.SparkContext.addFile" title="pyspark.ml.tuning.SparkContext.addFile"><code class="xref py py-obj docutils literal"><span class="pre">addFile</span></code></a>(path)</td>
<td>Add a file to be downloaded with this Spark job on every node.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.addPyFile.html#pyspark.ml.tuning.SparkContext.addPyFile" title="pyspark.ml.tuning.SparkContext.addPyFile"><code class="xref py py-obj docutils literal"><span class="pre">addPyFile</span></code></a>(path)</td>
<td>Add a .py or .zip dependency for all tasks to be executed on this SparkContext in the future.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.binaryFiles.html#pyspark.ml.tuning.SparkContext.binaryFiles" title="pyspark.ml.tuning.SparkContext.binaryFiles"><code class="xref py py-obj docutils literal"><span class="pre">binaryFiles</span></code></a>(path[,&nbsp;minPartitions])</td>
<td><div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental</p>
</div>
</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.binaryRecords.html#pyspark.ml.tuning.SparkContext.binaryRecords" title="pyspark.ml.tuning.SparkContext.binaryRecords"><code class="xref py py-obj docutils literal"><span class="pre">binaryRecords</span></code></a>(path,&nbsp;recordLength)</td>
<td><div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental</p>
</div>
</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.broadcast.html#pyspark.ml.tuning.SparkContext.broadcast" title="pyspark.ml.tuning.SparkContext.broadcast"><code class="xref py py-obj docutils literal"><span class="pre">broadcast</span></code></a>(value)</td>
<td>Broadcast a read-only variable to the cluster, returning a L{Broadcast&lt;pyspark.broadcast.Broadcast&gt;} object for reading it in distributed functions.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.cancelAllJobs.html#pyspark.ml.tuning.SparkContext.cancelAllJobs" title="pyspark.ml.tuning.SparkContext.cancelAllJobs"><code class="xref py py-obj docutils literal"><span class="pre">cancelAllJobs</span></code></a>()</td>
<td>Cancel all jobs that have been scheduled or are running.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.cancelJobGroup.html#pyspark.ml.tuning.SparkContext.cancelJobGroup" title="pyspark.ml.tuning.SparkContext.cancelJobGroup"><code class="xref py py-obj docutils literal"><span class="pre">cancelJobGroup</span></code></a>(groupId)</td>
<td>Cancel active jobs for the specified group.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.clearFiles.html#pyspark.ml.tuning.SparkContext.clearFiles" title="pyspark.ml.tuning.SparkContext.clearFiles"><code class="xref py py-obj docutils literal"><span class="pre">clearFiles</span></code></a>()</td>
<td>Clear the job&#8217;s list of files added by L{addFile} or L{addPyFile} so that they do not get downloaded to any new nodes.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.dump_profiles.html#pyspark.ml.tuning.SparkContext.dump_profiles" title="pyspark.ml.tuning.SparkContext.dump_profiles"><code class="xref py py-obj docutils literal"><span class="pre">dump_profiles</span></code></a>(path)</td>
<td>Dump the profile stats into directory <cite>path</cite></td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.emptyRDD.html#pyspark.ml.tuning.SparkContext.emptyRDD" title="pyspark.ml.tuning.SparkContext.emptyRDD"><code class="xref py py-obj docutils literal"><span class="pre">emptyRDD</span></code></a>()</td>
<td>Create an RDD that has no partitions or elements.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.getConf.html#pyspark.ml.tuning.SparkContext.getConf" title="pyspark.ml.tuning.SparkContext.getConf"><code class="xref py py-obj docutils literal"><span class="pre">getConf</span></code></a>()</td>
<td></td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.getLocalProperty.html#pyspark.ml.tuning.SparkContext.getLocalProperty" title="pyspark.ml.tuning.SparkContext.getLocalProperty"><code class="xref py py-obj docutils literal"><span class="pre">getLocalProperty</span></code></a>(key)</td>
<td>Get a local property set in this thread, or null if it is missing.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.getOrCreate.html#pyspark.ml.tuning.SparkContext.getOrCreate" title="pyspark.ml.tuning.SparkContext.getOrCreate"><code class="xref py py-obj docutils literal"><span class="pre">getOrCreate</span></code></a>([conf])</td>
<td>Get or instantiate a SparkContext and register it as a singleton object.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.hadoopFile.html#pyspark.ml.tuning.SparkContext.hadoopFile" title="pyspark.ml.tuning.SparkContext.hadoopFile"><code class="xref py py-obj docutils literal"><span class="pre">hadoopFile</span></code></a>(path,&nbsp;inputFormatClass,&nbsp;keyClass,&nbsp;...)</td>
<td>Read an &#8216;old&#8217; Hadoop InputFormat with arbitrary key and value class from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.hadoopRDD.html#pyspark.ml.tuning.SparkContext.hadoopRDD" title="pyspark.ml.tuning.SparkContext.hadoopRDD"><code class="xref py py-obj docutils literal"><span class="pre">hadoopRDD</span></code></a>(inputFormatClass,&nbsp;keyClass,&nbsp;valueClass)</td>
<td>Read an &#8216;old&#8217; Hadoop InputFormat with arbitrary key and value class, from an arbitrary Hadoop configuration, which is passed in as a Python dict.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.newAPIHadoopFile.html#pyspark.ml.tuning.SparkContext.newAPIHadoopFile" title="pyspark.ml.tuning.SparkContext.newAPIHadoopFile"><code class="xref py py-obj docutils literal"><span class="pre">newAPIHadoopFile</span></code></a>(path,&nbsp;inputFormatClass,&nbsp;...)</td>
<td>Read a &#8216;new API&#8217; Hadoop InputFormat with arbitrary key and value class from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.newAPIHadoopRDD.html#pyspark.ml.tuning.SparkContext.newAPIHadoopRDD" title="pyspark.ml.tuning.SparkContext.newAPIHadoopRDD"><code class="xref py py-obj docutils literal"><span class="pre">newAPIHadoopRDD</span></code></a>(inputFormatClass,&nbsp;keyClass,&nbsp;...)</td>
<td>Read a &#8216;new API&#8217; Hadoop InputFormat with arbitrary key and value class, from an arbitrary Hadoop configuration, which is passed in as a Python dict.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.parallelize.html#pyspark.ml.tuning.SparkContext.parallelize" title="pyspark.ml.tuning.SparkContext.parallelize"><code class="xref py py-obj docutils literal"><span class="pre">parallelize</span></code></a>(c[,&nbsp;numSlices])</td>
<td>Distribute a local Python collection to form an RDD.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.pickleFile.html#pyspark.ml.tuning.SparkContext.pickleFile" title="pyspark.ml.tuning.SparkContext.pickleFile"><code class="xref py py-obj docutils literal"><span class="pre">pickleFile</span></code></a>(name[,&nbsp;minPartitions])</td>
<td>Load an RDD previously saved using L{RDD.saveAsPickleFile} method.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.range.html#pyspark.ml.tuning.SparkContext.range" title="pyspark.ml.tuning.SparkContext.range"><code class="xref py py-obj docutils literal"><span class="pre">range</span></code></a>(start[,&nbsp;end,&nbsp;step,&nbsp;numSlices])</td>
<td>Create a new RDD of int containing elements from <cite>start</cite> to <cite>end</cite> (exclusive), increased by <cite>step</cite> every element.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.runJob.html#pyspark.ml.tuning.SparkContext.runJob" title="pyspark.ml.tuning.SparkContext.runJob"><code class="xref py py-obj docutils literal"><span class="pre">runJob</span></code></a>(rdd,&nbsp;partitionFunc[,&nbsp;partitions,&nbsp;...])</td>
<td>Executes the given partitionFunc on the specified set of partitions, returning the result as an array of elements.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.sequenceFile.html#pyspark.ml.tuning.SparkContext.sequenceFile" title="pyspark.ml.tuning.SparkContext.sequenceFile"><code class="xref py py-obj docutils literal"><span class="pre">sequenceFile</span></code></a>(path[,&nbsp;keyClass,&nbsp;valueClass,&nbsp;...])</td>
<td>Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.setCheckpointDir.html#pyspark.ml.tuning.SparkContext.setCheckpointDir" title="pyspark.ml.tuning.SparkContext.setCheckpointDir"><code class="xref py py-obj docutils literal"><span class="pre">setCheckpointDir</span></code></a>(dirName)</td>
<td>Set the directory under which RDDs are going to be checkpointed.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.setJobGroup.html#pyspark.ml.tuning.SparkContext.setJobGroup" title="pyspark.ml.tuning.SparkContext.setJobGroup"><code class="xref py py-obj docutils literal"><span class="pre">setJobGroup</span></code></a>(groupId,&nbsp;description[,&nbsp;...])</td>
<td>Assigns a group ID to all the jobs started by this thread until the group ID is set to a different value or cleared.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.setLocalProperty.html#pyspark.ml.tuning.SparkContext.setLocalProperty" title="pyspark.ml.tuning.SparkContext.setLocalProperty"><code class="xref py py-obj docutils literal"><span class="pre">setLocalProperty</span></code></a>(key,&nbsp;value)</td>
<td>Set a local property that affects jobs submitted from this thread, such as the Spark fair scheduler pool.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.setLogLevel.html#pyspark.ml.tuning.SparkContext.setLogLevel" title="pyspark.ml.tuning.SparkContext.setLogLevel"><code class="xref py py-obj docutils literal"><span class="pre">setLogLevel</span></code></a>(logLevel)</td>
<td>Control our logLevel.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.setSystemProperty.html#pyspark.ml.tuning.SparkContext.setSystemProperty" title="pyspark.ml.tuning.SparkContext.setSystemProperty"><code class="xref py py-obj docutils literal"><span class="pre">setSystemProperty</span></code></a>(key,&nbsp;value)</td>
<td>Set a Java system property, such as spark.executor.memory.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.show_profiles.html#pyspark.ml.tuning.SparkContext.show_profiles" title="pyspark.ml.tuning.SparkContext.show_profiles"><code class="xref py py-obj docutils literal"><span class="pre">show_profiles</span></code></a>()</td>
<td>Print the profile stats to stdout</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.sparkUser.html#pyspark.ml.tuning.SparkContext.sparkUser" title="pyspark.ml.tuning.SparkContext.sparkUser"><code class="xref py py-obj docutils literal"><span class="pre">sparkUser</span></code></a>()</td>
<td>Get SPARK_USER for user who is running SparkContext.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.statusTracker.html#pyspark.ml.tuning.SparkContext.statusTracker" title="pyspark.ml.tuning.SparkContext.statusTracker"><code class="xref py py-obj docutils literal"><span class="pre">statusTracker</span></code></a>()</td>
<td>Return <code class="xref py py-class docutils literal"><span class="pre">StatusTracker</span></code> object</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.stop.html#pyspark.ml.tuning.SparkContext.stop" title="pyspark.ml.tuning.SparkContext.stop"><code class="xref py py-obj docutils literal"><span class="pre">stop</span></code></a>()</td>
<td>Shut down the SparkContext.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.textFile.html#pyspark.ml.tuning.SparkContext.textFile" title="pyspark.ml.tuning.SparkContext.textFile"><code class="xref py py-obj docutils literal"><span class="pre">textFile</span></code></a>(name[,&nbsp;minPartitions,&nbsp;use_unicode])</td>
<td>Read a text file from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI, and return it as an RDD of Strings.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.union.html#pyspark.ml.tuning.SparkContext.union" title="pyspark.ml.tuning.SparkContext.union"><code class="xref py py-obj docutils literal"><span class="pre">union</span></code></a>(rdds)</td>
<td>Build the union of a list of RDDs.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.wholeTextFiles.html#pyspark.ml.tuning.SparkContext.wholeTextFiles" title="pyspark.ml.tuning.SparkContext.wholeTextFiles"><code class="xref py py-obj docutils literal"><span class="pre">wholeTextFiles</span></code></a>(path[,&nbsp;minPartitions,&nbsp;...])</td>
<td>Read a directory of text files from HDFS, a local file system (available on all nodes), or any  Hadoop-supported file system URI.</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="attributes">
<h2>3.9.2.10.2. Attributes<a class="headerlink" href="#attributes" title="Permalink to this headline">¶</a></h2>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.PACKAGE_EXTENSIONS.html#pyspark.ml.tuning.SparkContext.PACKAGE_EXTENSIONS" title="pyspark.ml.tuning.SparkContext.PACKAGE_EXTENSIONS"><code class="xref py py-obj docutils literal"><span class="pre">PACKAGE_EXTENSIONS</span></code></a></td>
<td></td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.applicationId.html#pyspark.ml.tuning.SparkContext.applicationId" title="pyspark.ml.tuning.SparkContext.applicationId"><code class="xref py py-obj docutils literal"><span class="pre">applicationId</span></code></a></td>
<td>A unique identifier for the Spark application.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.defaultMinPartitions.html#pyspark.ml.tuning.SparkContext.defaultMinPartitions" title="pyspark.ml.tuning.SparkContext.defaultMinPartitions"><code class="xref py py-obj docutils literal"><span class="pre">defaultMinPartitions</span></code></a></td>
<td>Default min number of partitions for Hadoop RDDs when not given by user</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.defaultParallelism.html#pyspark.ml.tuning.SparkContext.defaultParallelism" title="pyspark.ml.tuning.SparkContext.defaultParallelism"><code class="xref py py-obj docutils literal"><span class="pre">defaultParallelism</span></code></a></td>
<td>Default level of parallelism to use when not given by user (e.g.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.startTime.html#pyspark.ml.tuning.SparkContext.startTime" title="pyspark.ml.tuning.SparkContext.startTime"><code class="xref py py-obj docutils literal"><span class="pre">startTime</span></code></a></td>
<td>Return the epoch time when the Spark Context was started.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.ml.tuning.SparkContext.version.html#pyspark.ml.tuning.SparkContext.version" title="pyspark.ml.tuning.SparkContext.version"><code class="xref py py-obj docutils literal"><span class="pre">version</span></code></a></td>
<td>The version of Spark on which this application is running.</td>
</tr>
</tbody>
</table>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="pyspark.ml.tuning.SparkContext.__init__.html" class="btn btn-neutral float-right" title="3.9.2.10.1.1. pyspark.ml.tuning.SparkContext.__init__" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="pyspark.ml.tuning.Params.params.html" class="btn btn-neutral" title="3.9.2.9.2.1. pyspark.ml.tuning.Params.params" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../_static/copybutton.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>