

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>4.3.2.19. pyspark.mllib.clustering.RDD &mdash; PySpark API 1 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="PySpark API 1 documentation" href="../../index.html"/>
        <link rel="up" title="4.3. pyspark.mllib.clustering" href="../pyspark.mllib.clustering.html"/>
        <link rel="next" title="4.3.2.19.1.1. pyspark.mllib.clustering.RDD.__init__" href="pyspark.mllib.clustering.RDD.__init__.html"/>
        <link rel="prev" title="4.3.2.18.2.1. pyspark.mllib.clustering.PowerIterationClusteringModel.k" href="pyspark.mllib.clustering.PowerIterationClusteringModel.k.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> PySpark API
          

          
          </a>

          
            
            
              <div class="version">
                1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <p class="caption"><span class="caption-text">Table of Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../pyspark.html">1. pyspark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pyspark.sql.html">2. pyspark.sql</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pyspark.ml.html">3. pyspark.ml</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../pyspark.mllib.html">4. pyspark.mllib</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../pyspark.mllib.html">4.1. pyspark.mllib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pyspark.mllib.classification.html">4.2. pyspark.mllib.classification</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../pyspark.mllib.clustering.html">4.3. pyspark.mllib.clustering</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../pyspark.mllib.clustering.html#functions">4.3.1. Functions</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../pyspark.mllib.clustering.html#classes">4.3.2. Classes</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.BisectingKMeans.html">4.3.2.1. pyspark.mllib.clustering.BisectingKMeans</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.BisectingKMeansModel.html">4.3.2.2. pyspark.mllib.clustering.BisectingKMeansModel</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.DStream.html">4.3.2.3. pyspark.mllib.clustering.DStream</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.DenseVector.html">4.3.2.4. pyspark.mllib.clustering.DenseVector</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.GaussianMixture.html">4.3.2.5. pyspark.mllib.clustering.GaussianMixture</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.GaussianMixtureModel.html">4.3.2.6. pyspark.mllib.clustering.GaussianMixtureModel</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.JavaLoader.html">4.3.2.7. pyspark.mllib.clustering.JavaLoader</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.JavaModelWrapper.html">4.3.2.8. pyspark.mllib.clustering.JavaModelWrapper</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.JavaSaveable.html">4.3.2.9. pyspark.mllib.clustering.JavaSaveable</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.KMeans.html">4.3.2.10. pyspark.mllib.clustering.KMeans</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.KMeansModel.html">4.3.2.11. pyspark.mllib.clustering.KMeansModel</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.LDA.html">4.3.2.12. pyspark.mllib.clustering.LDA</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.LDAModel.html">4.3.2.13. pyspark.mllib.clustering.LDAModel</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.LabeledPoint.html">4.3.2.14. pyspark.mllib.clustering.LabeledPoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.Loader.html">4.3.2.15. pyspark.mllib.clustering.Loader</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.MultivariateGaussian.html">4.3.2.16. pyspark.mllib.clustering.MultivariateGaussian</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.PowerIterationClustering.html">4.3.2.17. pyspark.mllib.clustering.PowerIterationClustering</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.PowerIterationClusteringModel.html">4.3.2.18. pyspark.mllib.clustering.PowerIterationClusteringModel</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">4.3.2.19. pyspark.mllib.clustering.RDD</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.Saveable.html">4.3.2.20. pyspark.mllib.clustering.Saveable</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.SparkContext.html">4.3.2.21. pyspark.mllib.clustering.SparkContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.SparseVector.html">4.3.2.22. pyspark.mllib.clustering.SparseVector</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.StreamingKMeans.html">4.3.2.23. pyspark.mllib.clustering.StreamingKMeans</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark.mllib.clustering.StreamingKMeansModel.html">4.3.2.24. pyspark.mllib.clustering.StreamingKMeansModel</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../pyspark.mllib.evaluation.html">4.4. pyspark.mllib.evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pyspark.mllib.feature.html">4.5. pyspark.mllib.feature</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pyspark.mllib.fpm.html">4.6. pyspark.mllib.fpm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pyspark.mllib.linalg.html">4.7. pyspark.mllib.linalg</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pyspark.mllib.linalg.distributed.html">4.8. pyspark.mllib.linalg.distributed</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pyspark.mllib.random.html">4.9. pyspark.mllib.random</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pyspark.mllib.recommendation.html">4.10. pyspark.mllib.recommendation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pyspark.mllib.regression.html">4.11. pyspark.mllib.regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pyspark.mllib.stat.html">4.12. pyspark.mllib.stat</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pyspark.mllib.tree.html">4.13. pyspark.mllib.tree</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pyspark.mllib.util.html">4.14. pyspark.mllib.util</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../pyspark.streaming.html">5. pyspark.streaming</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">PySpark API</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          













<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../pyspark.mllib.html">4. pyspark.mllib</a> &raquo;</li>
        
          <li><a href="../pyspark.mllib.clustering.html">4.3. pyspark.mllib.clustering</a> &raquo;</li>
        
      <li>4.3.2.19. pyspark.mllib.clustering.RDD</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/generated/generated/pyspark.mllib.clustering.RDD.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="pyspark-mllib-clustering-rdd">
<h1>4.3.2.19. pyspark.mllib.clustering.RDD<a class="headerlink" href="#pyspark-mllib-clustering-rdd" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="pyspark.mllib.clustering.RDD">
<em class="property">class </em><code class="descclassname">pyspark.mllib.clustering.</code><code class="descname">RDD</code><span class="sig-paren">(</span><em>jrdd</em>, <em>ctx</em>, <em>jrdd_deserializer=AutoBatchedSerializer(PickleSerializer())</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/pyspark/rdd.html#RDD"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.mllib.clustering.RDD" title="Permalink to this definition">¶</a></dt>
<dd><p>A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.
Represents an immutable, partitioned collection of elements that can be
operated on in parallel.</p>
<dl class="method">
<dt id="pyspark.mllib.clustering.RDD.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>jrdd</em>, <em>ctx</em>, <em>jrdd_deserializer=AutoBatchedSerializer(PickleSerializer())</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/pyspark/rdd.html#RDD.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.mllib.clustering.RDD.__init__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<div class="section" id="methods">
<h2>4.3.2.19.1. Methods<a class="headerlink" href="#methods" title="Permalink to this headline">¶</a></h2>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.__init__.html#pyspark.mllib.clustering.RDD.__init__" title="pyspark.mllib.clustering.RDD.__init__"><code class="xref py py-obj docutils literal"><span class="pre">__init__</span></code></a>(jrdd,&nbsp;ctx[,&nbsp;jrdd_deserializer])</td>
<td></td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.aggregate.html#pyspark.mllib.clustering.RDD.aggregate" title="pyspark.mllib.clustering.RDD.aggregate"><code class="xref py py-obj docutils literal"><span class="pre">aggregate</span></code></a>(zeroValue,&nbsp;seqOp,&nbsp;combOp)</td>
<td>Aggregate the elements of each partition, and then the results for all</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.aggregateByKey.html#pyspark.mllib.clustering.RDD.aggregateByKey" title="pyspark.mllib.clustering.RDD.aggregateByKey"><code class="xref py py-obj docutils literal"><span class="pre">aggregateByKey</span></code></a>(zeroValue,&nbsp;seqFunc,&nbsp;combFunc)</td>
<td>Aggregate the values of each key, using given combine functions and a neutral &#8220;zero value&#8221;.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.cache.html#pyspark.mllib.clustering.RDD.cache" title="pyspark.mllib.clustering.RDD.cache"><code class="xref py py-obj docutils literal"><span class="pre">cache</span></code></a>()</td>
<td>Persist this RDD with the default storage level (C{MEMORY_ONLY}).</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.cartesian.html#pyspark.mllib.clustering.RDD.cartesian" title="pyspark.mllib.clustering.RDD.cartesian"><code class="xref py py-obj docutils literal"><span class="pre">cartesian</span></code></a>(other)</td>
<td>Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and C{b} is in C{other}.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.checkpoint.html#pyspark.mllib.clustering.RDD.checkpoint" title="pyspark.mllib.clustering.RDD.checkpoint"><code class="xref py py-obj docutils literal"><span class="pre">checkpoint</span></code></a>()</td>
<td>Mark this RDD for checkpointing.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.coalesce.html#pyspark.mllib.clustering.RDD.coalesce" title="pyspark.mllib.clustering.RDD.coalesce"><code class="xref py py-obj docutils literal"><span class="pre">coalesce</span></code></a>(numPartitions[,&nbsp;shuffle])</td>
<td>Return a new RDD that is reduced into <cite>numPartitions</cite> partitions.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.cogroup.html#pyspark.mllib.clustering.RDD.cogroup" title="pyspark.mllib.clustering.RDD.cogroup"><code class="xref py py-obj docutils literal"><span class="pre">cogroup</span></code></a>(other[,&nbsp;numPartitions])</td>
<td>For each key k in C{self} or C{other}, return a resulting RDD that contains a tuple with the list of values for that key in C{self} as well as C{other}.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.collect.html#pyspark.mllib.clustering.RDD.collect" title="pyspark.mllib.clustering.RDD.collect"><code class="xref py py-obj docutils literal"><span class="pre">collect</span></code></a>()</td>
<td>Return a list that contains all of the elements in this RDD.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.collectAsMap.html#pyspark.mllib.clustering.RDD.collectAsMap" title="pyspark.mllib.clustering.RDD.collectAsMap"><code class="xref py py-obj docutils literal"><span class="pre">collectAsMap</span></code></a>()</td>
<td>Return the key-value pairs in this RDD to the master as a dictionary.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.combineByKey.html#pyspark.mllib.clustering.RDD.combineByKey" title="pyspark.mllib.clustering.RDD.combineByKey"><code class="xref py py-obj docutils literal"><span class="pre">combineByKey</span></code></a>(createCombiner,&nbsp;mergeValue,&nbsp;...)</td>
<td>Generic function to combine the elements for each key using a custom set of aggregation functions.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.count.html#pyspark.mllib.clustering.RDD.count" title="pyspark.mllib.clustering.RDD.count"><code class="xref py py-obj docutils literal"><span class="pre">count</span></code></a>()</td>
<td>Return the number of elements in this RDD.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.countApprox.html#pyspark.mllib.clustering.RDD.countApprox" title="pyspark.mllib.clustering.RDD.countApprox"><code class="xref py py-obj docutils literal"><span class="pre">countApprox</span></code></a>(timeout[,&nbsp;confidence])</td>
<td><div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental</p>
</div>
</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.countApproxDistinct.html#pyspark.mllib.clustering.RDD.countApproxDistinct" title="pyspark.mllib.clustering.RDD.countApproxDistinct"><code class="xref py py-obj docutils literal"><span class="pre">countApproxDistinct</span></code></a>([relativeSD])</td>
<td><div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental</p>
</div>
</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.countByKey.html#pyspark.mllib.clustering.RDD.countByKey" title="pyspark.mllib.clustering.RDD.countByKey"><code class="xref py py-obj docutils literal"><span class="pre">countByKey</span></code></a>()</td>
<td>Count the number of elements for each key, and return the result to the master as a dictionary.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.countByValue.html#pyspark.mllib.clustering.RDD.countByValue" title="pyspark.mllib.clustering.RDD.countByValue"><code class="xref py py-obj docutils literal"><span class="pre">countByValue</span></code></a>()</td>
<td>Return the count of each unique value in this RDD as a dictionary of (value, count) pairs.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.distinct.html#pyspark.mllib.clustering.RDD.distinct" title="pyspark.mllib.clustering.RDD.distinct"><code class="xref py py-obj docutils literal"><span class="pre">distinct</span></code></a>([numPartitions])</td>
<td>Return a new RDD containing the distinct elements in this RDD.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.filter.html#pyspark.mllib.clustering.RDD.filter" title="pyspark.mllib.clustering.RDD.filter"><code class="xref py py-obj docutils literal"><span class="pre">filter</span></code></a>(f)</td>
<td>Return a new RDD containing only the elements that satisfy a predicate.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.first.html#pyspark.mllib.clustering.RDD.first" title="pyspark.mllib.clustering.RDD.first"><code class="xref py py-obj docutils literal"><span class="pre">first</span></code></a>()</td>
<td>Return the first element in this RDD.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.flatMap.html#pyspark.mllib.clustering.RDD.flatMap" title="pyspark.mllib.clustering.RDD.flatMap"><code class="xref py py-obj docutils literal"><span class="pre">flatMap</span></code></a>(f[,&nbsp;preservesPartitioning])</td>
<td>Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.flatMapValues.html#pyspark.mllib.clustering.RDD.flatMapValues" title="pyspark.mllib.clustering.RDD.flatMapValues"><code class="xref py py-obj docutils literal"><span class="pre">flatMapValues</span></code></a>(f)</td>
<td>Pass each value in the key-value pair RDD through a flatMap function without changing the keys; this also retains the original RDD&#8217;s partitioning.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.fold.html#pyspark.mllib.clustering.RDD.fold" title="pyspark.mllib.clustering.RDD.fold"><code class="xref py py-obj docutils literal"><span class="pre">fold</span></code></a>(zeroValue,&nbsp;op)</td>
<td>Aggregate the elements of each partition, and then the results for all</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.foldByKey.html#pyspark.mllib.clustering.RDD.foldByKey" title="pyspark.mllib.clustering.RDD.foldByKey"><code class="xref py py-obj docutils literal"><span class="pre">foldByKey</span></code></a>(zeroValue,&nbsp;func[,&nbsp;numPartitions,&nbsp;...])</td>
<td>Merge the values for each key using an associative function &#8220;func&#8221; and a neutral &#8220;zeroValue&#8221; which may be added to the result an arbitrary number of times, and must not change the result (e.g., 0 for addition, or 1 for multiplication.).</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.foreach.html#pyspark.mllib.clustering.RDD.foreach" title="pyspark.mllib.clustering.RDD.foreach"><code class="xref py py-obj docutils literal"><span class="pre">foreach</span></code></a>(f)</td>
<td>Applies a function to all elements of this RDD.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.foreachPartition.html#pyspark.mllib.clustering.RDD.foreachPartition" title="pyspark.mllib.clustering.RDD.foreachPartition"><code class="xref py py-obj docutils literal"><span class="pre">foreachPartition</span></code></a>(f)</td>
<td>Applies a function to each partition of this RDD.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.fullOuterJoin.html#pyspark.mllib.clustering.RDD.fullOuterJoin" title="pyspark.mllib.clustering.RDD.fullOuterJoin"><code class="xref py py-obj docutils literal"><span class="pre">fullOuterJoin</span></code></a>(other[,&nbsp;numPartitions])</td>
<td>Perform a right outer join of C{self} and C{other}.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.getCheckpointFile.html#pyspark.mllib.clustering.RDD.getCheckpointFile" title="pyspark.mllib.clustering.RDD.getCheckpointFile"><code class="xref py py-obj docutils literal"><span class="pre">getCheckpointFile</span></code></a>()</td>
<td>Gets the name of the file to which this RDD was checkpointed</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.getNumPartitions.html#pyspark.mllib.clustering.RDD.getNumPartitions" title="pyspark.mllib.clustering.RDD.getNumPartitions"><code class="xref py py-obj docutils literal"><span class="pre">getNumPartitions</span></code></a>()</td>
<td>Returns the number of partitions in RDD</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.getStorageLevel.html#pyspark.mllib.clustering.RDD.getStorageLevel" title="pyspark.mllib.clustering.RDD.getStorageLevel"><code class="xref py py-obj docutils literal"><span class="pre">getStorageLevel</span></code></a>()</td>
<td>Get the RDD&#8217;s current storage level.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.glom.html#pyspark.mllib.clustering.RDD.glom" title="pyspark.mllib.clustering.RDD.glom"><code class="xref py py-obj docutils literal"><span class="pre">glom</span></code></a>()</td>
<td>Return an RDD created by coalescing all elements within each partition into a list.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.groupBy.html#pyspark.mllib.clustering.RDD.groupBy" title="pyspark.mllib.clustering.RDD.groupBy"><code class="xref py py-obj docutils literal"><span class="pre">groupBy</span></code></a>(f[,&nbsp;numPartitions,&nbsp;partitionFunc])</td>
<td>Return an RDD of grouped items.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.groupByKey.html#pyspark.mllib.clustering.RDD.groupByKey" title="pyspark.mllib.clustering.RDD.groupByKey"><code class="xref py py-obj docutils literal"><span class="pre">groupByKey</span></code></a>([numPartitions,&nbsp;partitionFunc])</td>
<td>Group the values for each key in the RDD into a single sequence.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.groupWith.html#pyspark.mllib.clustering.RDD.groupWith" title="pyspark.mllib.clustering.RDD.groupWith"><code class="xref py py-obj docutils literal"><span class="pre">groupWith</span></code></a>(other,&nbsp;*others)</td>
<td>Alias for cogroup but with support for multiple RDDs.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.histogram.html#pyspark.mllib.clustering.RDD.histogram" title="pyspark.mllib.clustering.RDD.histogram"><code class="xref py py-obj docutils literal"><span class="pre">histogram</span></code></a>(buckets)</td>
<td>Compute a histogram using the provided buckets.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.id.html#pyspark.mllib.clustering.RDD.id" title="pyspark.mllib.clustering.RDD.id"><code class="xref py py-obj docutils literal"><span class="pre">id</span></code></a>()</td>
<td>A unique ID for this RDD (within its SparkContext).</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.intersection.html#pyspark.mllib.clustering.RDD.intersection" title="pyspark.mllib.clustering.RDD.intersection"><code class="xref py py-obj docutils literal"><span class="pre">intersection</span></code></a>(other)</td>
<td>Return the intersection of this RDD and another one.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.isCheckpointed.html#pyspark.mllib.clustering.RDD.isCheckpointed" title="pyspark.mllib.clustering.RDD.isCheckpointed"><code class="xref py py-obj docutils literal"><span class="pre">isCheckpointed</span></code></a>()</td>
<td>Return whether this RDD has been checkpointed or not</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.isEmpty.html#pyspark.mllib.clustering.RDD.isEmpty" title="pyspark.mllib.clustering.RDD.isEmpty"><code class="xref py py-obj docutils literal"><span class="pre">isEmpty</span></code></a>()</td>
<td>Returns true if and only if the RDD contains no elements at all.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.join.html#pyspark.mllib.clustering.RDD.join" title="pyspark.mllib.clustering.RDD.join"><code class="xref py py-obj docutils literal"><span class="pre">join</span></code></a>(other[,&nbsp;numPartitions])</td>
<td>Return an RDD containing all pairs of elements with matching keys in C{self} and C{other}.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.keyBy.html#pyspark.mllib.clustering.RDD.keyBy" title="pyspark.mllib.clustering.RDD.keyBy"><code class="xref py py-obj docutils literal"><span class="pre">keyBy</span></code></a>(f)</td>
<td>Creates tuples of the elements in this RDD by applying C{f}.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.keys.html#pyspark.mllib.clustering.RDD.keys" title="pyspark.mllib.clustering.RDD.keys"><code class="xref py py-obj docutils literal"><span class="pre">keys</span></code></a>()</td>
<td>Return an RDD with the keys of each tuple.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.leftOuterJoin.html#pyspark.mllib.clustering.RDD.leftOuterJoin" title="pyspark.mllib.clustering.RDD.leftOuterJoin"><code class="xref py py-obj docutils literal"><span class="pre">leftOuterJoin</span></code></a>(other[,&nbsp;numPartitions])</td>
<td>Perform a left outer join of C{self} and C{other}.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.lookup.html#pyspark.mllib.clustering.RDD.lookup" title="pyspark.mllib.clustering.RDD.lookup"><code class="xref py py-obj docutils literal"><span class="pre">lookup</span></code></a>(key)</td>
<td>Return the list of values in the RDD for key <cite>key</cite>.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.map.html#pyspark.mllib.clustering.RDD.map" title="pyspark.mllib.clustering.RDD.map"><code class="xref py py-obj docutils literal"><span class="pre">map</span></code></a>(f[,&nbsp;preservesPartitioning])</td>
<td>Return a new RDD by applying a function to each element of this RDD.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.mapPartitions.html#pyspark.mllib.clustering.RDD.mapPartitions" title="pyspark.mllib.clustering.RDD.mapPartitions"><code class="xref py py-obj docutils literal"><span class="pre">mapPartitions</span></code></a>(f[,&nbsp;preservesPartitioning])</td>
<td>Return a new RDD by applying a function to each partition of this RDD.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.mapPartitionsWithIndex.html#pyspark.mllib.clustering.RDD.mapPartitionsWithIndex" title="pyspark.mllib.clustering.RDD.mapPartitionsWithIndex"><code class="xref py py-obj docutils literal"><span class="pre">mapPartitionsWithIndex</span></code></a>(f[,&nbsp;...])</td>
<td>Return a new RDD by applying a function to each partition of this RDD, while tracking the index of the original partition.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.mapPartitionsWithSplit.html#pyspark.mllib.clustering.RDD.mapPartitionsWithSplit" title="pyspark.mllib.clustering.RDD.mapPartitionsWithSplit"><code class="xref py py-obj docutils literal"><span class="pre">mapPartitionsWithSplit</span></code></a>(f[,&nbsp;...])</td>
<td>Deprecated: use mapPartitionsWithIndex instead.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.mapValues.html#pyspark.mllib.clustering.RDD.mapValues" title="pyspark.mllib.clustering.RDD.mapValues"><code class="xref py py-obj docutils literal"><span class="pre">mapValues</span></code></a>(f)</td>
<td>Pass each value in the key-value pair RDD through a map function without changing the keys; this also retains the original RDD&#8217;s partitioning.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.max.html#pyspark.mllib.clustering.RDD.max" title="pyspark.mllib.clustering.RDD.max"><code class="xref py py-obj docutils literal"><span class="pre">max</span></code></a>([key])</td>
<td>Find the maximum item in this RDD.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.mean.html#pyspark.mllib.clustering.RDD.mean" title="pyspark.mllib.clustering.RDD.mean"><code class="xref py py-obj docutils literal"><span class="pre">mean</span></code></a>()</td>
<td>Compute the mean of this RDD&#8217;s elements.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.meanApprox.html#pyspark.mllib.clustering.RDD.meanApprox" title="pyspark.mllib.clustering.RDD.meanApprox"><code class="xref py py-obj docutils literal"><span class="pre">meanApprox</span></code></a>(timeout[,&nbsp;confidence])</td>
<td><div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental</p>
</div>
</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.min.html#pyspark.mllib.clustering.RDD.min" title="pyspark.mllib.clustering.RDD.min"><code class="xref py py-obj docutils literal"><span class="pre">min</span></code></a>([key])</td>
<td>Find the minimum item in this RDD.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.name.html#pyspark.mllib.clustering.RDD.name" title="pyspark.mllib.clustering.RDD.name"><code class="xref py py-obj docutils literal"><span class="pre">name</span></code></a>()</td>
<td>Return the name of this RDD.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.partitionBy.html#pyspark.mllib.clustering.RDD.partitionBy" title="pyspark.mllib.clustering.RDD.partitionBy"><code class="xref py py-obj docutils literal"><span class="pre">partitionBy</span></code></a>(numPartitions[,&nbsp;partitionFunc])</td>
<td>Return a copy of the RDD partitioned using the specified partitioner.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.persist.html#pyspark.mllib.clustering.RDD.persist" title="pyspark.mllib.clustering.RDD.persist"><code class="xref py py-obj docutils literal"><span class="pre">persist</span></code></a>([storageLevel])</td>
<td>Set this RDD&#8217;s storage level to persist its values across operations after the first time it is computed.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.pipe.html#pyspark.mllib.clustering.RDD.pipe" title="pyspark.mllib.clustering.RDD.pipe"><code class="xref py py-obj docutils literal"><span class="pre">pipe</span></code></a>(command[,&nbsp;env,&nbsp;checkCode])</td>
<td>Return an RDD created by piping elements to a forked external process.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.randomSplit.html#pyspark.mllib.clustering.RDD.randomSplit" title="pyspark.mllib.clustering.RDD.randomSplit"><code class="xref py py-obj docutils literal"><span class="pre">randomSplit</span></code></a>(weights[,&nbsp;seed])</td>
<td>Randomly splits this RDD with the provided weights.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.reduce.html#pyspark.mllib.clustering.RDD.reduce" title="pyspark.mllib.clustering.RDD.reduce"><code class="xref py py-obj docutils literal"><span class="pre">reduce</span></code></a>(f)</td>
<td>Reduces the elements of this RDD using the specified commutative and associative binary operator.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.reduceByKey.html#pyspark.mllib.clustering.RDD.reduceByKey" title="pyspark.mllib.clustering.RDD.reduceByKey"><code class="xref py py-obj docutils literal"><span class="pre">reduceByKey</span></code></a>(func[,&nbsp;numPartitions,&nbsp;partitionFunc])</td>
<td>Merge the values for each key using an associative and commutative reduce function.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.reduceByKeyLocally.html#pyspark.mllib.clustering.RDD.reduceByKeyLocally" title="pyspark.mllib.clustering.RDD.reduceByKeyLocally"><code class="xref py py-obj docutils literal"><span class="pre">reduceByKeyLocally</span></code></a>(func)</td>
<td>Merge the values for each key using an associative and commutative reduce function, but return the results immediately to the master as a dictionary.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.repartition.html#pyspark.mllib.clustering.RDD.repartition" title="pyspark.mllib.clustering.RDD.repartition"><code class="xref py py-obj docutils literal"><span class="pre">repartition</span></code></a>(numPartitions)</td>
<td>Return a new RDD that has exactly numPartitions partitions.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.repartitionAndSortWithinPartitions.html#pyspark.mllib.clustering.RDD.repartitionAndSortWithinPartitions" title="pyspark.mllib.clustering.RDD.repartitionAndSortWithinPartitions"><code class="xref py py-obj docutils literal"><span class="pre">repartitionAndSortWithinPartitions</span></code></a>([...])</td>
<td>Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.rightOuterJoin.html#pyspark.mllib.clustering.RDD.rightOuterJoin" title="pyspark.mllib.clustering.RDD.rightOuterJoin"><code class="xref py py-obj docutils literal"><span class="pre">rightOuterJoin</span></code></a>(other[,&nbsp;numPartitions])</td>
<td>Perform a right outer join of C{self} and C{other}.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.sample.html#pyspark.mllib.clustering.RDD.sample" title="pyspark.mllib.clustering.RDD.sample"><code class="xref py py-obj docutils literal"><span class="pre">sample</span></code></a>(withReplacement,&nbsp;fraction[,&nbsp;seed])</td>
<td>Return a sampled subset of this RDD.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.sampleByKey.html#pyspark.mllib.clustering.RDD.sampleByKey" title="pyspark.mllib.clustering.RDD.sampleByKey"><code class="xref py py-obj docutils literal"><span class="pre">sampleByKey</span></code></a>(withReplacement,&nbsp;fractions[,&nbsp;seed])</td>
<td>Return a subset of this RDD sampled by key (via stratified sampling).</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.sampleStdev.html#pyspark.mllib.clustering.RDD.sampleStdev" title="pyspark.mllib.clustering.RDD.sampleStdev"><code class="xref py py-obj docutils literal"><span class="pre">sampleStdev</span></code></a>()</td>
<td>Compute the sample standard deviation of this RDD&#8217;s elements (which corrects for bias in estimating the standard deviation by dividing by N-1 instead of N).</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.sampleVariance.html#pyspark.mllib.clustering.RDD.sampleVariance" title="pyspark.mllib.clustering.RDD.sampleVariance"><code class="xref py py-obj docutils literal"><span class="pre">sampleVariance</span></code></a>()</td>
<td>Compute the sample variance of this RDD&#8217;s elements (which corrects for bias in estimating the variance by dividing by N-1 instead of N).</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.saveAsHadoopDataset.html#pyspark.mllib.clustering.RDD.saveAsHadoopDataset" title="pyspark.mllib.clustering.RDD.saveAsHadoopDataset"><code class="xref py py-obj docutils literal"><span class="pre">saveAsHadoopDataset</span></code></a>(conf[,&nbsp;keyConverter,&nbsp;...])</td>
<td>Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file system, using the old Hadoop OutputFormat API (mapred package).</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.saveAsHadoopFile.html#pyspark.mllib.clustering.RDD.saveAsHadoopFile" title="pyspark.mllib.clustering.RDD.saveAsHadoopFile"><code class="xref py py-obj docutils literal"><span class="pre">saveAsHadoopFile</span></code></a>(path,&nbsp;outputFormatClass[,&nbsp;...])</td>
<td>Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file system, using the old Hadoop OutputFormat API (mapred package).</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.saveAsNewAPIHadoopDataset.html#pyspark.mllib.clustering.RDD.saveAsNewAPIHadoopDataset" title="pyspark.mllib.clustering.RDD.saveAsNewAPIHadoopDataset"><code class="xref py py-obj docutils literal"><span class="pre">saveAsNewAPIHadoopDataset</span></code></a>(conf[,&nbsp;...])</td>
<td>Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file system, using the new Hadoop OutputFormat API (mapreduce package).</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.saveAsNewAPIHadoopFile.html#pyspark.mllib.clustering.RDD.saveAsNewAPIHadoopFile" title="pyspark.mllib.clustering.RDD.saveAsNewAPIHadoopFile"><code class="xref py py-obj docutils literal"><span class="pre">saveAsNewAPIHadoopFile</span></code></a>(path,&nbsp;outputFormatClass)</td>
<td>Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file system, using the new Hadoop OutputFormat API (mapreduce package).</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.saveAsPickleFile.html#pyspark.mllib.clustering.RDD.saveAsPickleFile" title="pyspark.mllib.clustering.RDD.saveAsPickleFile"><code class="xref py py-obj docutils literal"><span class="pre">saveAsPickleFile</span></code></a>(path[,&nbsp;batchSize])</td>
<td>Save this RDD as a SequenceFile of serialized objects.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.saveAsSequenceFile.html#pyspark.mllib.clustering.RDD.saveAsSequenceFile" title="pyspark.mllib.clustering.RDD.saveAsSequenceFile"><code class="xref py py-obj docutils literal"><span class="pre">saveAsSequenceFile</span></code></a>(path[,&nbsp;compressionCodecClass])</td>
<td>Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file system, using the L{org.apache.hadoop.io.Writable} types that we convert from the RDD&#8217;s key and value types.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.saveAsTextFile.html#pyspark.mllib.clustering.RDD.saveAsTextFile" title="pyspark.mllib.clustering.RDD.saveAsTextFile"><code class="xref py py-obj docutils literal"><span class="pre">saveAsTextFile</span></code></a>(path[,&nbsp;compressionCodecClass])</td>
<td>Save this RDD as a text file, using string representations of elements.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.setName.html#pyspark.mllib.clustering.RDD.setName" title="pyspark.mllib.clustering.RDD.setName"><code class="xref py py-obj docutils literal"><span class="pre">setName</span></code></a>(name)</td>
<td>Assign a name to this RDD.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.sortBy.html#pyspark.mllib.clustering.RDD.sortBy" title="pyspark.mllib.clustering.RDD.sortBy"><code class="xref py py-obj docutils literal"><span class="pre">sortBy</span></code></a>(keyfunc[,&nbsp;ascending,&nbsp;numPartitions])</td>
<td>Sorts this RDD by the given keyfunc</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.sortByKey.html#pyspark.mllib.clustering.RDD.sortByKey" title="pyspark.mllib.clustering.RDD.sortByKey"><code class="xref py py-obj docutils literal"><span class="pre">sortByKey</span></code></a>([ascending,&nbsp;numPartitions,&nbsp;keyfunc])</td>
<td>Sorts this RDD, which is assumed to consist of (key, value) pairs.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.stats.html#pyspark.mllib.clustering.RDD.stats" title="pyspark.mllib.clustering.RDD.stats"><code class="xref py py-obj docutils literal"><span class="pre">stats</span></code></a>()</td>
<td>Return a L{StatCounter} object that captures the mean, variance and count of the RDD&#8217;s elements in one operation.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.stdev.html#pyspark.mllib.clustering.RDD.stdev" title="pyspark.mllib.clustering.RDD.stdev"><code class="xref py py-obj docutils literal"><span class="pre">stdev</span></code></a>()</td>
<td>Compute the standard deviation of this RDD&#8217;s elements.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.subtract.html#pyspark.mllib.clustering.RDD.subtract" title="pyspark.mllib.clustering.RDD.subtract"><code class="xref py py-obj docutils literal"><span class="pre">subtract</span></code></a>(other[,&nbsp;numPartitions])</td>
<td>Return each value in C{self} that is not contained in C{other}.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.subtractByKey.html#pyspark.mllib.clustering.RDD.subtractByKey" title="pyspark.mllib.clustering.RDD.subtractByKey"><code class="xref py py-obj docutils literal"><span class="pre">subtractByKey</span></code></a>(other[,&nbsp;numPartitions])</td>
<td>Return each (key, value) pair in C{self} that has no pair with matching key in C{other}.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.sum.html#pyspark.mllib.clustering.RDD.sum" title="pyspark.mllib.clustering.RDD.sum"><code class="xref py py-obj docutils literal"><span class="pre">sum</span></code></a>()</td>
<td>Add up the elements in this RDD.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.sumApprox.html#pyspark.mllib.clustering.RDD.sumApprox" title="pyspark.mllib.clustering.RDD.sumApprox"><code class="xref py py-obj docutils literal"><span class="pre">sumApprox</span></code></a>(timeout[,&nbsp;confidence])</td>
<td><div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental</p>
</div>
</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.take.html#pyspark.mllib.clustering.RDD.take" title="pyspark.mllib.clustering.RDD.take"><code class="xref py py-obj docutils literal"><span class="pre">take</span></code></a>(num)</td>
<td>Take the first num elements of the RDD.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.takeOrdered.html#pyspark.mllib.clustering.RDD.takeOrdered" title="pyspark.mllib.clustering.RDD.takeOrdered"><code class="xref py py-obj docutils literal"><span class="pre">takeOrdered</span></code></a>(num[,&nbsp;key])</td>
<td>Get the N elements from a RDD ordered in ascending order or as specified by the optional key function.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.takeSample.html#pyspark.mllib.clustering.RDD.takeSample" title="pyspark.mllib.clustering.RDD.takeSample"><code class="xref py py-obj docutils literal"><span class="pre">takeSample</span></code></a>(withReplacement,&nbsp;num[,&nbsp;seed])</td>
<td>Return a fixed-size sampled subset of this RDD.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.toDebugString.html#pyspark.mllib.clustering.RDD.toDebugString" title="pyspark.mllib.clustering.RDD.toDebugString"><code class="xref py py-obj docutils literal"><span class="pre">toDebugString</span></code></a>()</td>
<td>A description of this RDD and its recursive dependencies for debugging.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.toLocalIterator.html#pyspark.mllib.clustering.RDD.toLocalIterator" title="pyspark.mllib.clustering.RDD.toLocalIterator"><code class="xref py py-obj docutils literal"><span class="pre">toLocalIterator</span></code></a>()</td>
<td>Return an iterator that contains all of the elements in this RDD.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.top.html#pyspark.mllib.clustering.RDD.top" title="pyspark.mllib.clustering.RDD.top"><code class="xref py py-obj docutils literal"><span class="pre">top</span></code></a>(num[,&nbsp;key])</td>
<td>Get the top N elements from a RDD.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.treeAggregate.html#pyspark.mllib.clustering.RDD.treeAggregate" title="pyspark.mllib.clustering.RDD.treeAggregate"><code class="xref py py-obj docutils literal"><span class="pre">treeAggregate</span></code></a>(zeroValue,&nbsp;seqOp,&nbsp;combOp[,&nbsp;depth])</td>
<td>Aggregates the elements of this RDD in a multi-level tree pattern.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.treeReduce.html#pyspark.mllib.clustering.RDD.treeReduce" title="pyspark.mllib.clustering.RDD.treeReduce"><code class="xref py py-obj docutils literal"><span class="pre">treeReduce</span></code></a>(f[,&nbsp;depth])</td>
<td>Reduces the elements of this RDD in a multi-level tree pattern.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.union.html#pyspark.mllib.clustering.RDD.union" title="pyspark.mllib.clustering.RDD.union"><code class="xref py py-obj docutils literal"><span class="pre">union</span></code></a>(other)</td>
<td>Return the union of this RDD and another one.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.unpersist.html#pyspark.mllib.clustering.RDD.unpersist" title="pyspark.mllib.clustering.RDD.unpersist"><code class="xref py py-obj docutils literal"><span class="pre">unpersist</span></code></a>()</td>
<td>Mark the RDD as non-persistent, and remove all blocks for it from memory and disk.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.values.html#pyspark.mllib.clustering.RDD.values" title="pyspark.mllib.clustering.RDD.values"><code class="xref py py-obj docutils literal"><span class="pre">values</span></code></a>()</td>
<td>Return an RDD with the values of each tuple.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.variance.html#pyspark.mllib.clustering.RDD.variance" title="pyspark.mllib.clustering.RDD.variance"><code class="xref py py-obj docutils literal"><span class="pre">variance</span></code></a>()</td>
<td>Compute the variance of this RDD&#8217;s elements.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.zip.html#pyspark.mllib.clustering.RDD.zip" title="pyspark.mllib.clustering.RDD.zip"><code class="xref py py-obj docutils literal"><span class="pre">zip</span></code></a>(other)</td>
<td>Zips this RDD with another one, returning key-value pairs with the first element in each RDD second element in each RDD, etc.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.zipWithIndex.html#pyspark.mllib.clustering.RDD.zipWithIndex" title="pyspark.mllib.clustering.RDD.zipWithIndex"><code class="xref py py-obj docutils literal"><span class="pre">zipWithIndex</span></code></a>()</td>
<td>Zips this RDD with its element indices.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.zipWithUniqueId.html#pyspark.mllib.clustering.RDD.zipWithUniqueId" title="pyspark.mllib.clustering.RDD.zipWithUniqueId"><code class="xref py py-obj docutils literal"><span class="pre">zipWithUniqueId</span></code></a>()</td>
<td>Zips this RDD with generated unique Long ids.</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="attributes">
<h2>4.3.2.19.2. Attributes<a class="headerlink" href="#attributes" title="Permalink to this headline">¶</a></h2>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="pyspark.mllib.clustering.RDD.context.html#pyspark.mllib.clustering.RDD.context" title="pyspark.mllib.clustering.RDD.context"><code class="xref py py-obj docutils literal"><span class="pre">context</span></code></a></td>
<td>The L{SparkContext} that this RDD was created on.</td>
</tr>
</tbody>
</table>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="pyspark.mllib.clustering.RDD.__init__.html" class="btn btn-neutral float-right" title="4.3.2.19.1.1. pyspark.mllib.clustering.RDD.__init__" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="pyspark.mllib.clustering.PowerIterationClusteringModel.k.html" class="btn btn-neutral" title="4.3.2.18.2.1. pyspark.mllib.clustering.PowerIterationClusteringModel.k" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>